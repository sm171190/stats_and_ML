{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"C://Users/saurmisr/Downloads/titanic.csv\"\n",
    "data = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for missing data for all relevant features and the label\n",
    "def checkMissingData(X,cols):\n",
    "    '''Checks if columns have missing values\n",
    "    INPUT: X - a 2D numpy array, cols - list/tuple/array of column names\n",
    "    OUTPUT: Prints the number of missing entries in each column of cols\n",
    "    '''\n",
    "    total_missing_entries = 0\n",
    "    for col in cols:\n",
    "        total_missing_entries += data[col].isna().sum()\n",
    "        print(f'column {col}: Found {total_missing_entries} missing entries.')\n",
    "    \n",
    "    print(f'Found a total of {total_missing_entries} missing entries in the data, corresponding to columns {cols}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pY(Y, k):\n",
    "    '''Returns the ML-estimated probability that Y is in class k : P(Y=k)\n",
    "    '''\n",
    "    N=Y.size\n",
    "    return np.sum(Y==k)/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findMaxFeatureSetSize(X,discreteFeatureIds):\n",
    "    '''\n",
    "    Finds J, the maximum among all the different number of values each discrete feature can take.\n",
    "    INPUT : X, an mxn numpy array of data, \n",
    "            discreteFeatureIds - a list of the column numbers of X corresponding to discrete features\n",
    "    '''\n",
    "    J=0\n",
    "    for featureId in discreteFeatureIds:\n",
    "        N_distinct_feature_values = np.size(np.unique(X[:,featureId]))\n",
    "        if J< N_distinct_feature_values:\n",
    "            J = N_distinct_feature_values\n",
    "    return J\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatureSpace(X,discreteFeatureIds):\n",
    "    '''\n",
    "    Returns the feature space for discrete features\n",
    "    INPUT : X, an mxn numpy array of data, \n",
    "            discreteFeatureIds - a list of the column numbers of X corresponding to discrete features\n",
    "    OUTPUT: S is an nDxJ numpy array, where S[i,:] are all the possible values that the ith discrete feature takes on.\n",
    "    '''\n",
    "    nD=len(discreteFeatureIds)\n",
    "    J = findMaxFeatureSetSize(X,discreteFeatureIds)\n",
    "    S = np.zeros((n,J))\n",
    "    for featureId in range(nD):\n",
    "        unique_feature_values  = np.unique(X[:,discreteFeatureIds[featureId]])\n",
    "        N_values = unique_feature_values.size\n",
    "        S[featureId,:N_values] = unique_feature_values\n",
    "    \n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabulate_pX_given_Y(X,Y,beta_values,discreteFeatureIds):\n",
    "    '''\n",
    "    Returns the ML-estimated conditional probabilities Theta(i,j,k) = [P(Xi=xj|Y=k)], Theta(i,j,k) = P(Xi=xj|Y=k)\n",
    "    for the discrete-valued features\n",
    "     INPUT : X, an mxn numpy array of data, \n",
    "             Y, an mx1 numpy array of labels,\n",
    "             beta_values : a list of beta parameters for MAP estimation \n",
    "             discreteFeatureIds - a list of the column numbers of X corresponding to discrete features\n",
    "    OUTPUT: Theta is an nDxJxK numpy array where Thetak(i,j,k) = P(Xi=xj|Y=k)  0<=i<nD , 0<=j<J , 0<=k<K\n",
    "    '''     \n",
    "    classes = np.unique(Y)\n",
    "    K = classes.size\n",
    "    nD = len(discreteFeatureIds)\n",
    "    J= findMaxFeatureSetSize(X,discreteFeatureIds)\n",
    "    featureSpace = getFeatureSpace(X,discreteFeatureIds)\n",
    "    Theta  = np.zeros((nD,J,K))\n",
    "    for k in range(K):  \n",
    "        row_ids_where_Y_equals_k = np.argwhere(Y==classes[k]) # Find the row numbers where Y=y \n",
    "        Ny = row_ids_where_Y_equals_k.shape[0] \n",
    "        for i in range(nD):\n",
    "            for m in range(Ny):\n",
    "                rowId = row_ids_where_Y_equals_k[m][0]\n",
    "                j = np.where(featureSpace[i,:]==X[rowId,discreteFeatureIds[i]])[0][0]\n",
    "                Theta[i,j,k]+=1\n",
    "        Theta[:,:,k]/=Ny\n",
    "        \n",
    "    return Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSampleMeans(X,Y, continuousFeatureIds):\n",
    "    '''\n",
    "    Returns the ML-estimated sample mean E[Xi|Y=k], k=0,...,K-1 for continuous features Xi, i=1,...,nC\n",
    "    INPUT : X, an mxn numpy array of data, \n",
    "            Y, an mx1 numpy array of labels,             \n",
    "            continuousFeatureIds - a list of the column numbers of X corresponding to continuous features\n",
    "    OUTPUT: means is an nCxK numpy array where mu(i,k) = E[Xi|Y=k] k=0,...,K-1 for continuous features Xi, i=1,...,nC\n",
    "    ''' \n",
    "    classes = np.unique(Y)\n",
    "    K = classes.size\n",
    "    nC = len(continuousFeatureIds)\n",
    "    means = np.zeros((nC,K))\n",
    "    for k in range(K):\n",
    "        for i in range(nC):\n",
    "            X_filtered = X[(Y==classes[k])]\n",
    "            means[i,k] = X_filtered[:,continuousFeatureIds[i]].mean()\n",
    "    return means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSampleVariances(X,Y, continuousFeatureIds):\n",
    "    '''\n",
    "    Returns the ML-estimated sample Variance E[(Xi-mui)|Y=k], k=0,...,K-1 for continuous features Xi, i=1,...,nC\n",
    "    INPUT : X, an mxn numpy array of data, \n",
    "            Y, an mx1 numpy array of labels,             \n",
    "            continuousFeatureIds - a list of the column numbers of X corresponding to continuous features\n",
    "    OUTPUT:\n",
    "    var is an nCxK numpy array where mu(i,k) = E[(Xi-mui)|Y=k] k=0,...,K-1 for continuous features Xi, i=1,...,nC\n",
    "    ''' \n",
    "    classes = np.unique(Y)\n",
    "    K = classes.size\n",
    "    nC = len(continuousFeatureIds)\n",
    "    var  = np.zeros((nC,K))\n",
    "    for k in range(K):\n",
    "        for i in range(nC):\n",
    "            X_filtered = X[(Y==classes[k])]\n",
    "            var[i,k] = X_filtered[:,continuousFeatureIds[i]].var(ddof=1)\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScoreForContinuousFeature(x,mean,variance):\n",
    "    '''\n",
    "    Finds the class-conditional score for a continuous feature\n",
    "    INPUT : x - the value of the continuous feature\n",
    "            mean - the class-conditional sample mean for the continuous feature\n",
    "            variance - the class-conditional sample variance for the continuous feature\n",
    "    OUPUT : log(P(X=x|Y=k)), for some continuous feature X. Assumes X~Normal(mean,variance)\n",
    "    '''\n",
    "    \n",
    "    return (-np.log(np.sqrt(np.pi*variance)) - ((x-mean)**2)/(2*variance))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findObservedDataScore(x,Theta,logProb,featureSpace,discreteFeatureIds,continuousFeatureIds,means,variances):\n",
    "    '''\n",
    "    Finds the score for one observation (X1,...,Xn)\n",
    "    INPUT : x , a 1xn numpy array of observation\n",
    "            Theta, the nDxJxK numpy array of conditional probabilities for discrete features P(Xi=xj|Y=k)\n",
    "            logProb, a 1xK list of the log of the respective class proabilities\n",
    "            featureSpace , the nDxJ numpy array of the feature space of the discrete features\n",
    "            discreteFeatureIds, a list of the column numbers of x corresponding to discrete features\n",
    "            continuousFeatureIds - a list of the column numbers of x corresponding to continuous features\n",
    "            means - an nCxK numpy array of the class-conditional means for each continuous feature\n",
    "            variances - an nCxK numpy array of the class-conditional variances for each continuous feature \n",
    "            \n",
    "    OUTPUT: scores  = [P(X1=x1|Y=1)...P(Xn=xn|Y=1).P(Y=1),...,P(X1=x1|Y=K)...P(Xn=xn|Y=K).P(Y=K)], a 1xK numpy array\n",
    "    '''\n",
    "    n,J,K = Theta.shape\n",
    "    scores = np.zeros(K)\n",
    "    for k in range(K):\n",
    "        score=0\n",
    "        # Counting the scores from the discrete-valued features\n",
    "        for i in discreteFeatureIds:\n",
    "            j = np.where(featureSpace[i,:]==x[i])[0][0]\n",
    "            score+=np.log(Theta[i,j,k])\n",
    "        \n",
    "        # Counting the scores from the continuous-valued features\n",
    "        N_cont_features = len(continuousFeatureIds)\n",
    "        for i in range(N_cont_features):\n",
    "#             print(f'i:{i}')\n",
    "#             print('Input:')\n",
    "#             print(f'{x[continuousFeatureIds[i]]}')\n",
    "#             print(f'{means[i,k]}')\n",
    "#             print(f'{variances[i,k]}')\n",
    "#             print(f'{(x[continuousFeatureIds[i]],means[i,k],variances[i,k])}')\n",
    "            score += getScoreForContinuousFeature(x[continuousFeatureIds[i]],means[i,k],variances[i,k])\n",
    "                \n",
    "        \n",
    "        score+=logProb[k]\n",
    "        scores[k]=score\n",
    "    \n",
    "    return scores              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,Theta,logProb,featureSpace,discreteFeatureIds,continuousFeatureIds,means,variances):\n",
    "    '''\n",
    "    Returns class predictions for each observation\n",
    "    INPUT : X , a mxn numpy array of observations\n",
    "            Theta, the nDxJxK numpy array of conditional probabilities for discrete features P(Xi=xj|Y=k)\n",
    "            logProb, a 1xK list of the log of the respective class proabilities\n",
    "            featureSpace , the nDxJ numpy array of the feature space of the discrete features\n",
    "            discreteFeatureIds, a list of the column numbers of x corresponding to discrete features\n",
    "            continuousFeatureIds - a list of the column numbers of x corresponding to continuous features\n",
    "            means - an nCxK numpy array of the class-conditional means for each continuous feature\n",
    "            variances - an nCxK numpy array of the class-conditional variances for each continuous feature \n",
    "            \n",
    "    OUTPUT:\n",
    "    predictions - a 1xm numpy array of class predictions\n",
    "    '''\n",
    "    M = X.shape[0]\n",
    "    predictions = np.zeros((M,1))\n",
    "    for m in range(M):\n",
    "        scores = findObservedDataScore(X[m,:],Theta,logProb,featureSpace,discreteFeatureIds,continuousFeatureIds,means,variances)\n",
    "#         if scores[0]>scores[1]:\n",
    "#             predictions[m] = 0\n",
    "#         else:\n",
    "#             predictions[m] = 1\n",
    "        predictions[m]=np.argmax(scores)\n",
    "    return predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getErrorRate(yPred,yActual):        \n",
    "    return np.sum(np.abs(yPred[:,0]-yActual))/yPred.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLOBAL PARAMETERS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logProb = np.log10(pY(yTrain,0)),np.log10(pY(yTrain,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE SET 1 ASSUMPTIONS:\n",
    "#### 1. We shall only consider the Passenger Class, Sex, Siblings/Spouses and Parents/Children columns as relevant indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yTrain = data['Survived'].to_numpy()\n",
    "PclassCol = data['Pclass'].to_numpy()\n",
    "siblings_or_spousesCol = data['Siblings/Spouses Aboard'].to_numpy()\n",
    "parents_or_childrenCol = data['Parents/Children Aboard'].to_numpy()\n",
    "sexCol = data['Sex'].apply(lambda X: 1 if X=='male' else 0).to_numpy()\n",
    "XTrainFeatureSet1 = np.column_stack((PclassCol,siblings_or_spousesCol,parents_or_childrenCol,sexCol))\n",
    "(m,n) = XTrainFeatureSet1.shape\n",
    "print(f'Feature Set 1 size (m,n): {(m,n)}')\n",
    "columnNames_1 = ['Survived','Pclass','Sex','Siblings/Spouses Aboard','Parents/Children Aboard']\n",
    "checkMissingData(data,columnNames_1)\n",
    "\n",
    "discrete_Features_1 = [0,1,2,3]\n",
    "continuous_features_1 = []\n",
    "beta_vals = None\n",
    "Theta1 = tabulate_pX_given_Y(XTrainFeatureSet1,yTrain,beta_vals,discrete_Features_1)\n",
    "S1 = getFeatureSpace(XTrainFeatureSet1,discrete_Features_1)\n",
    "(means_1,variances_1) = (None,None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE SET 2 ASSUMPTIONS:\n",
    "#### 1. We shall only consider the Passenger Class, Sex,Age,Siblings/Spouses and Parents/Children columns as relevant indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: This assumes that Feature Set 1 has been created\n",
    "AgeCol = data['Age']\n",
    "XTrainFeatureSet2 = np.column_stack((PclassCol,siblings_or_spousesCol,parents_or_childrenCol,sexCol,AgeCol))\n",
    "(m,n) = XTrainFeatureSet2.shape\n",
    "print(f'Feature Set 2 size (m,n): {(m,n)}')\n",
    "columnNames_2 = ['Survived','Pclass','Sex','Age','Siblings/Spouses Aboard','Parents/Children Aboard']\n",
    "checkMissingData(data,columnNames_2)\n",
    "discrete_Features_2 = [0,1,2,3]\n",
    "continuous_features_2 = [4]\n",
    "beta_vals = None\n",
    "Theta2 = tabulate_pX_given_Y(XTrainFeatureSet2,yTrain,beta_vals,discrete_Features_2)\n",
    "means_2 = getSampleMeans(XTrainFeatureSet2,yTrain,continuous_features_2)\n",
    "variances_2 = getSampleVariances(XTrainFeatureSet2,yTrain,continuous_features_2)\n",
    "S2 = getFeatureSpace(XTrainFeatureSet2,discrete_Features_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE SET 3 ASSUMPTIONS:\n",
    "#### 1. We shall only consider the Passenger Class, Sex,Age,Siblings/Spouses, Parents/Children and Fare columns as relevant indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: This assumes that Feature Sets 1 and 2 have been created\n",
    "FareCol = data['Fare']\n",
    "XTrainFeatureSet3 = np.column_stack((PclassCol,siblings_or_spousesCol,parents_or_childrenCol,sexCol,AgeCol,FareCol))\n",
    "(m,n) = XTrainFeatureSet3.shape\n",
    "print(f'Feature Set 3 size (m,n): {(m,n)}')\n",
    "columnNames_3 = ['Survived','Pclass','Sex','Age','Siblings/Spouses Aboard','Parents/Children Aboard','Fare']\n",
    "checkMissingData(data,columnNames_3)\n",
    "discrete_Features_3 = [0,1,2,3]\n",
    "continuous_features_3 = [4,5]\n",
    "beta_vals = None\n",
    "Theta3 = tabulate_pX_given_Y(XTrainFeatureSet3,yTrain,beta_vals,discrete_Features_3)\n",
    "means_3 = getSampleMeans(XTrainFeatureSet3,yTrain,continuous_features_3)\n",
    "variances_3 = getSampleVariances(XTrainFeatureSet3,yTrain,continuous_features_3)\n",
    "S3 = getFeatureSpace(XTrainFeatureSet3,discrete_Features_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_1 = predict(XTrainFeatureSet1,Theta1,logProb,S1,discrete_Features_1,continuous_features_1,means_1,variances_1)\n",
    "y_pred_train_2 = predict(XTrainFeatureSet2,Theta2,logProb,S2,discrete_Features_2,continuous_features_2,means_2,variances_2)\n",
    "y_pred_train_3 = predict(XTrainFeatureSet3,Theta3,logProb,S3,discrete_Features_3,continuous_features_3,means_3,variances_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ERROR RATES:')\n",
    "print(f'FEATURE SET 1 : {getErrorRate(y_pred_train_1,yTrain)} ')\n",
    "print(f'FEATURE SET 2 : {getErrorRate(y_pred_train_2,yTrain)} ')\n",
    "print(f'FEATURE SET 3 : {getErrorRate(y_pred_train_3,yTrain)} ')\n",
    "print('SUCCESS RATES:')\n",
    "print(f'FEATURE SET 1 : {1-getErrorRate(y_pred_train_1,yTrain)} ')\n",
    "print(f'FEATURE SET 2 : {1-getErrorRate(y_pred_train_2,yTrain)} ')\n",
    "print(f'FEATURE SET 3 : {1-getErrorRate(y_pred_train_3,yTrain)} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SK-LEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "X = data.drop(labels=['Survived','Name'],axis=1)\n",
    "X['Sex']=X['Sex'].apply(lambda X:0 if X=='male' else 0)\n",
    "y = data['Survived']\n",
    "clf.fit(X,y)\n",
    "y_pred_GNB = clf.predict(X)\n",
    "error_rate_GNB = np.sum(np.abs(y_pred_GNB-y))/y.size\n",
    "print(f'Error Rate: {error_rate_GNB}')\n",
    "print(f'Success Rate: {1-error_rate_GNB}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X,y)\n",
    "y_pred_LR = clf.predict(X)\n",
    "error_rate_LR = np.sum(np.abs(y_pred_LR-y))/y.size\n",
    "print(f'Error Rate: {error_rate_LR}')\n",
    "print(f'Success Rate: {1-error_rate_LR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
